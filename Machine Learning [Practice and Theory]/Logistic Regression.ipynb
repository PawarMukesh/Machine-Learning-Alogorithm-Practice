{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrhdKIt9qCZ5"
   },
   "source": [
    "## Introduction\n",
    "In linear regression, the type of data we deal with is quantitative, whereas we use classification models to deal with qualitative data or categorical data. The algorithms used for solving a classification problem first predict the probability of each of the categories of the qualitative variables, as the basis for making the classification. And, as the probabilities are continuous numbers, classification using probabilities also behave like regression methods. \n",
    "Logistic regression is one such type of classification model which is used to classify the dependent variable into two or more classes or categories. \n",
    "\n",
    "###### Why don’t we use Linear regression for classification problems?\n",
    "\n",
    "\n",
    "Let’s suppose you took a survey and noted the response of each person as satisfied, neutral or Not satisfied.\n",
    "Let’s map each category:\n",
    "\n",
    "Satisfied – 2\n",
    "\n",
    "Neutral – 1\n",
    "\n",
    "Not Satisfied – 0\n",
    "\n",
    "But this doesn’t mean that the gap between Not satisfied and Neutral is same as Neutral and satisfied. There is no mathematical significance of these mapping. We can also map the categories like:\n",
    "\n",
    "Satisfied – 0\n",
    "\n",
    "Neutral – 1\n",
    "\n",
    "Not Satisfied – 2\n",
    "\n",
    "It’s completely fine to choose the above mapping. If we apply linear regression to both the type of mappings, we will get different sets of predictions. Also, we can get prediction values like 1.2, 0.8, 2.3 etc. which makes no sense for categorical values. So, there is no normal method to convert qualitative data into quantitative data for use in linear regression.\n",
    "Although, for binary classification, i.e. when there only two categorical values, using the least square method can give decent results. Suppose we have two categories Black and White and we map them as follows:\n",
    "\n",
    "Black – 0\n",
    "\n",
    "White - 1 \n",
    "\n",
    "We can assign predicted values for both the categories such as Y> 0.5 goes to class white and vice versa.\n",
    "Although, there will be some predictions for which the value can be greater than 1 or less than 0 making them hard to classify in any class. Nevertheless, linear regression can work decently for binary classification but not that well for multi-class classification. \n",
    "Hence, we use classification methods for dealing with such problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_NCj8WLqCZ9"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is one such regression algorithm which can be used for performing classification problems. It calculates the probability that a given value belongs to a specific class. If the probability is more than 50%, it assigns the value in that particular class else if the probability is less than 50%, the value is assigned to the other class. Therefore, we can say that logistic regression acts as a binary classifier.\n",
    "\n",
    "###### Working of a Logistic Model\n",
    "For linear regression, the model is defined by:\n",
    "$y = \\beta_0 + \\beta_1x  $       - (i)\n",
    "\n",
    "and for logistic regression, we calculate probability, i.e. y is the probability of a given variable x belonging to a certain class. Thus, it is obvious that the value of y should lie between 0 and 1.\n",
    "\n",
    "But, when we use equation(i) to calculate probability, we would get values less than 0 as well as greater than 1. That doesn’t make any sense\n",
    ".\n",
    "So, we need to use such an equation which always gives values between 0 and 1, as we desire while calculating the probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkGR1Fv-qCZ-"
   },
   "source": [
    "#### Sigmoid function \n",
    "\n",
    "We use the sigmoid function as the underlying function in Logistic regression. Mathematically and graphically, it is shown as:\n",
    "\n",
    "<img src=\"sigmoid.PNG\" width=\"300\">\n",
    "\n",
    "**Why do we use the Sigmoid Function?**\n",
    "\n",
    "1)\tThe sigmoid function’s range is bounded between 0 and 1. Thus it’s useful in calculating the probability for the  Logistic function.\n",
    "2)\t It’s derivative is easy to calculate than other functions which is useful during gradient descent calculation.\n",
    "3)\tIt is a simple way of introducing non-linearity to the model.\n",
    "\n",
    "Although there are other functions as well, which can be used, but sigmoid is the most common function used for logistic regression. We will talk about the rest of the functions in the neural network section.\n",
    "\n",
    "The logistic function is given as:\n",
    "\n",
    "<img src=\"logistic_function.PNG\" width=\"300\">\n",
    "\n",
    "Let’s see some manipulation with the logistic function: \n",
    "\n",
    "<img src=\"manip1.PNG\" width=\"300\">\n",
    "\n",
    "We can see that the logit function is linear in terms with x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAYZkYhHqCZ-"
   },
   "source": [
    "**Prediction**\n",
    "\n",
    "<img src=\"prediction.PNG\" width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLbJZmjLqCZ_"
   },
   "source": [
    "## Evaluation of a Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL_KCM7LqCZ_"
   },
   "source": [
    "In machine learning, once we have a result of the classification problem, how do we measure how accurate our classification is?\n",
    "For a  regression problem, we have different metrics like R Squared score, Mean Squared Error etc. what are the metrics to measure the credibility of a classification model?\n",
    "\n",
    "Metrics\n",
    "In a regression problem, the accuracy is generally measured in terms of the difference in the actual values and the predicted values.\n",
    "In a classification problem, the credibility of the model is measured using the confusion matrix generated, i.e., how accurately the true positives and true negatives were predicted.\n",
    "The different metrics used for this purpose are:\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- F1 Score\n",
    "- Specifity\n",
    "- AUC( Area Under the Curve)\n",
    "- ROC(Receiver Operator Characteristic)\n",
    "- Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHcIVIjbqCaA"
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A typical confusion matrix looks like the figure shown.\n",
    "\n",
    "<img src=\"confusionMatrix.PNG\" width=\"300\">\n",
    "\n",
    "Where the terms have the meaning:\n",
    "\n",
    "\t__True Positive(TP):__ A result that was predicted as positive by the classification model and also is positive\n",
    "\n",
    "\t__True Negative(TN):__ A result that was predicted as negative by the classification model and also is negative\n",
    "\n",
    "\t__False Positive(FP):__ A result that was predicted as positive by the classification model but actually is negative\n",
    "\n",
    "\t__False Negative(FN):__ A result that was predicted as negative by the classification model but actually is positive.\n",
    "\n",
    "The Credibility of the model is based on how many correct predictions did the model do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9tyVh7fqCaB"
   },
   "source": [
    "### Accuracy\n",
    "The mathematical formula is :\n",
    "\n",
    "   __Accuracy__= $ \\frac{ (TP+TN)}{(TP+TN+FP+FN)} $\n",
    "    \n",
    "Or, it can be said that it’s defined as the total number of correct classifications divided by the total number of classifications.\n",
    "Its is not the correct for inbalanc data beacause its always show you high accurancy becoz its bais to the high count data in binary classification\n",
    "becoz its not calculate the error / its won't count the error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6cLmt4cqCaB"
   },
   "source": [
    "#### Recall or Sensitivity\n",
    "The mathematical formula is:\n",
    "\n",
    "   __Recall__= $ \\frac{ TP}{(TP+FN)} $\n",
    "\n",
    "Or, as the name suggests, it is a measure of: from the total number of positive results how many positives were correctly predicted by the model.\n",
    "\n",
    "It shows how relevant the model is, in terms of positive results only.\n",
    "\n",
    "Consider a classification model , the model gave 50 correct predictions(TP) but failed to identify 200 cancer patients(FN). Recall in that case will be:\n",
    "\n",
    "Recall=$ \\frac {50}{(50+200)} $= 0.2 (The model was able to recall only 20% of the cancer patients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO5ITCBlqCaC"
   },
   "source": [
    "### Precision\n",
    "\n",
    "Precision is a measure of amongst all the positive predictions, how many of them were actually positive. Mathematically,\n",
    "\n",
    "Precision=$ \\frac {TP}{(TP+FP)} $\n",
    "\n",
    "Let’s suppose in the previous example, the model identified 50 people as cancer patients(TP) but also raised a  false alarm for 100 patients(FP). Hence,\n",
    "\n",
    "Precision=$ \\frac {50}{(50+100)} $=0.33 (The model only has a precision of 33%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvwwErkDqCaC"
   },
   "source": [
    "### But we have a problem!!\n",
    "\n",
    "As evident from the previous example, the model had a very high Accuracy but performed poorly in terms of Precision and Recall. So, necessarily _Accuracy_ is not the metric to use for evaluating the model in this case.\n",
    "\n",
    "Imagine a scenario, where the requirement was that the model recalled all the defaulters who did not pay back the loan. Suppose there were 10 such defaulters and to recall those 10 defaulters, and the model gave you 20 results out of which only the 10 are the actual defaulters. Now, the recall of the model is 100%, but the precision goes down to 50%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x99V_GbAqCaD"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### A Trade-off?\n",
    "\n",
    "<img src=\"tradeoff.PNG\" width=\"300\">\n",
    "\n",
    "As observed from the graph, with an increase in the Recall, there is a drop in Precision of the model.\n",
    "\n",
    "So the question is - what to go for? Precision or Recall?\n",
    "\n",
    "Well, the answer is: it depends on the business requirement.\n",
    "\n",
    "For example, if you are predicting cancer, you need a 100 % recall. But suppose you are predicting whether a person is innocent or not, you need 100% precision.\n",
    "\n",
    "Can we maximise both at the same time? No\n",
    "\n",
    "So, there is a need for a better metric then?\n",
    "\n",
    "Yes. And it’s called an _F1 Score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXItOE5XqCaD"
   },
   "source": [
    "### F1 Score\n",
    "\n",
    "From the previous examples, it is clear that we need a metric that considers both Precision and Recall for evaluating a model. One such metric is the F1 score.\n",
    "\n",
    "F1 score is defined as the harmonic mean of Precision and Recall. \n",
    "\n",
    "The mathematical formula is:\n",
    "        F1 score= $ \\frac {2*((Precision*Recall)}{(Precision+Recall))} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj74wseaqCaE"
   },
   "source": [
    "### Specificity or True Negative Rate\n",
    "\n",
    "This represents how specific is the model while predicting the True Negatives.\n",
    "Mathematically,\n",
    "\n",
    "   Specificity=$ \\frac {TN}{(TN+FP)} $\n",
    "Or, it can be said that it quantifies the total number of negatives predicted by the model with respect to the total number of actual negative or non favorable outcomes.\n",
    "\n",
    "Similarly, False Positive rate can be defined as:  (1- specificity)\n",
    "Or,  $ \\frac {FP}{(TN+FP)} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMHp_s7rqCaE"
   },
   "source": [
    "### ROC(Receiver Operator Characteristic)\n",
    "\n",
    "We know that the classification algorithms work on the concept of probability of occurrence of the possible outcomes. A probability value lies between 0 and 1. Zero means that there is no probability of occurrence and one means that the occurrence is certain.\n",
    "\n",
    "But while working with real-time data, it has been observed that we seldom get a perfect 0 or 1 value. Instead of that, we get different decimal values lying between 0 and 1. Now the question is if we are not getting binary probability values how are we actually determining the class in our classification problem?\n",
    "\n",
    "There comes the concept of Threshold. A threshold is set, any probability value below the threshold is a negative outcome, and anything more than the threshold is a favourable or the positive outcome. For Example, if the threshold is 0.5, any probability value below 0.5 means a negative or an unfavourable outcome and any value above 0.5 indicates a positive or favourable outcome. \n",
    "\n",
    "Now, the question is, what should be an ideal threshold?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1egTEjrcqCaF"
   },
   "source": [
    "The following diagram shows a typical logistic regression curve.\n",
    "<img src=\"logisticRegression.PNG\" width=\"300\">\n",
    "\n",
    "* The horizontal lines represent the various values of thresholds ranging from 0 to 1.\n",
    "* Let’s suppose our classification problem was to identify the obese people from the given data.\n",
    "* The green markers represent obese people and the red markers represent the non-obese people.\n",
    "* Our confusion matrix will depend on the value of the threshold chosen by us.\n",
    "* For Example, if 0.25 is the threshold then\n",
    "        TP(actually obese)=3\n",
    "        TN(Not obese)=2\n",
    "        FP(Not obese but predicted obese)=2(the two red squares above the 0.25 line)\n",
    "        FN(Obese but predicted as not obese )=1(Green circle below 0.25line  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSJMelvEqCaF"
   },
   "source": [
    "A typical ROC curve looks like the following figure.\n",
    "<img src=\"ROC.PNG\" width=\"300\">\n",
    "\n",
    "* Mathematically, it represents the various confusion matrices for various thresholds. Each black dot is one confusion matrix.\n",
    "* The green dotted line represents the scenario when the true positive rate equals the false positive rate.\n",
    "* As evident from the curve, as we move from the rightmost dot towards left, after a certain threshold, the false positive rate decreases.\n",
    "* After some time, the false positive rate becomes zero.\n",
    "* The point encircled in green is the best point as it predicts all the values correctly and keeps the False positive as a minimum.\n",
    "* But that is not a rule of thumb. Based on the requirement, we need to select the point of a threshold.\n",
    "* The ROC curve answers our question of which threshold to choose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeyH61AyqCaF"
   },
   "source": [
    "### But we have a confusion!!\n",
    "\n",
    "Let’s suppose that we used different classification algorithms, and different ROCs for the corresponding algorithms have been plotted.\n",
    "The question is: which algorithm to choose now?\n",
    "The answer is to calculate the area under each ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89J1KbFJqCaF"
   },
   "source": [
    "#### AUC(Area Under Curve)\n",
    "\n",
    "<img src=\"AUC.PNG\" width=\"300\">\n",
    "\n",
    "* It helps us to choose the best model amongst the models for which we have plotted the ROC curves\n",
    "* The best model is the one which encompasses the maximum area under it.\n",
    "* In the adjacent diagram, amongst the two curves, the model that resulted in the red one should be chosen as it clearly covers more area than the blue one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_3qanOZqCaG"
   },
   "source": [
    "#### When to use recall and when to you precision\n",
    "\n",
    "* We have thousands of free customers registering in our website every week. The call center team wants to call them all, but it is imposible, so they ask me to select those with good chances to be a buyer (with high temperature is how we refer to them). We don't care to call a guy that is not going to buy (so precision is not important) but for us is very important that all of them with high temperature are always in my selection, so they don't go without buying. That means that my model needs to have a high recall, no matter if the precision goes to hell.\n",
    "\n",
    "#### Advantages of Logisitic Regression\n",
    "\n",
    "* It is very simple and easy to implement.\n",
    "* The output is more informative than other classification algorithms\n",
    "* It expresses the relationship between independent and dependent variables\n",
    "* Very effective with linearly seperable data\n",
    "\n",
    "#### Disadvantages of Logisitic Regression\n",
    "\n",
    "* Not effective with data which are not linearly seperable \n",
    "* Not as powerful as other classification models\n",
    "* Multiclass classifications are much easier to do with other algorithms than logisitic regression\n",
    "* It can only predict categorical outcomes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq_trb63qCaG"
   },
   "source": [
    " ## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1828,
     "status": "ok",
     "timestamp": 1625052253266,
     "user": {
      "displayName": "Shubhangi Sakarkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjiJriRrUWKSeoxmYnVoL2uz2i6E3RLOwZgeqHG=s64",
      "userId": "12180749557274197061"
     },
     "user_tz": 420
    },
    "id": "Mxz4KP5wqCaG"
   },
   "outputs": [],
   "source": [
    "#Let's start with importing necessary libraries\n",
    "\n",
    "import pandas as pd# reading the file other necessary operation  \n",
    "import numpy as np# from that we can get mean median and other operation\n",
    "from sklearn.preprocessing import StandardScaler # for scaling the data \n",
    "from sklearn.linear_model  import LogisticRegression # importing logoistic regression\n",
    "from sklearn.model_selection import train_test_split # for splitting the data in to trainning and testing \n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score #metric to check model performance\n",
    "import matplotlib.pyplot as plt # visualization library , analysis of data \n",
    "import seaborn as sns # visualization library , analysis of data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI06dSuoqCaH"
   },
   "source": [
    "# Businesscase:-To predict whether a patient will have diabetes or not??\n",
    "## Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1625052259183,
     "user": {
      "displayName": "Shubhangi Sakarkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjiJriRrUWKSeoxmYnVoL2uz2i6E3RLOwZgeqHG=s64",
      "userId": "12180749557274197061"
     },
     "user_tz": 420
    },
    "id": "mqBRbWCsqCaH",
    "outputId": "934690be-8a0c-4151-a99f-6794098b67be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"diabetes1.csv\") # Reading the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ZhGjDdqCaJ"
   },
   "source": [
    "## Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()#it will give you first  5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bajUtDcQqCaJ",
    "outputId": "58ad4904-7ac3-49e4-f8b1-5e9565333854"
   },
   "outputs": [],
   "source": [
    "data.tail()#it will give you last 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RnM13f6qCaI",
    "outputId": "b4a2c3ff-3033-4cc1-ad41-c1d79f881f41"
   },
   "outputs": [],
   "source": [
    "data.info()# To check  data type and  non null value of all columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZfst1RYqCaJ",
    "outputId": "efef102b-ff7c-4a58-cdc3-ad250b604376"
   },
   "outputs": [],
   "source": [
    "data.describe()#used to view some basic statistical details like percentile, mean, std etc. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ZhT5I--FqCaK"
   },
   "source": [
    "## write down the conclusion from describe.\n",
    "Glucose,Bloodpressure,Skinthickness,insulin,bmi cannot have 0 values.This isbcase of data corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPKe0biRqCaJ"
   },
   "outputs": [],
   "source": [
    "## Task:-Do the all feature analysis with the help of google.(Healthline)\n",
    "## What is the feature?\n",
    "## How features are impacting the target variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPpEWbgHqCaK"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUq_SrXLqCaK"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7dr5c5vqCaK",
    "outputId": "deea430a-0026-4ac4-ca2d-227ef89d97f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sweetviz as sv #  library for univariant analysis\n",
    "\n",
    "my_report = sv.analyze(data)## pass the original dataframe\n",
    "\n",
    "my_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What does insights from data means?\n",
    "# Insights means information from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate  Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "       'BMI', 'DiabetesPedigreeFunction', 'Age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25), facecolor='white')#To set canvas \n",
    "plotnumber = 1#counter\n",
    "\n",
    "for column in data1:#accessing the columns \n",
    "    if plotnumber<=16 :\n",
    "        ax = plt.subplot(4,4,plotnumber)\n",
    "        sns.histplot(x=data1[column],hue=data.Outcome)\n",
    "        plt.xlabel(column,fontsize=20)#assign name to x-axis and set font-20\n",
    "        plt.ylabel('Outcome',fontsize=20)\n",
    "    plotnumber+=1#counter increment\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps to perform in Feature engineering/data preprocessing\n",
    "\n",
    "1)Check missing values.Check for corrupted values if any.\n",
    "2)Convert categorical variable into numerical\n",
    "3)Handle outlier.\n",
    "4)Scale the data\n",
    "5)Transformation of data\n",
    "6)Balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "War36D8IqCaM",
    "outputId": "28cf508f-9920-4466-dbf0-3f5312f3fa6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get the missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMWpWHoQqCaM"
   },
   "source": [
    "### It seems that there are no missing values in our data. Great, let's see the distribution of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pslRpYqzqCaM",
    "outputId": "ec2f7ff9-3fbd-4131-8e49-3fd207051300"
   },
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')#defining  canvas size\n",
    "plotnumber = 1 #maintian count for graph\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=9 :# as there are 9 columns in the data\n",
    "        ax = plt.subplot(3,3,plotnumber)# plotting 9 graphs (3-rows,3-columns) ,plotnumber is for count \n",
    "        sns.distplot(data[column])#plotting dist plot to know distribution\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iFRqxVOqCaM"
   },
   "source": [
    "We can see there is some skewness in the data, let's deal with data.\n",
    "\n",
    "Also, we can see there few data for columns Glucose, Insulin, skin thickness, BMI and Blood Pressure which have value as 0. That's not possible. You can do a quick search to see that one cannot have 0 values for these.\n",
    "Let's deal with that. we can either remove such data or simply replace it with their respective mean values.\n",
    "Let's do the latter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YJaHUFDqCaN",
    "outputId": "453f46c9-459e-494e-d3e3-cc77c47ed819"
   },
   "outputs": [],
   "source": [
    "data.loc[data['BMI']==0].head() # How many rows have BMI=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcLfXilvqCaN",
    "outputId": "132dc9a0-14b2-46e0-ba4e-dcfdacb071db"
   },
   "outputs": [],
   "source": [
    "data['BMI'].mean()#bmi column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1625052277516,
     "user": {
      "displayName": "Shubhangi Sakarkar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjiJriRrUWKSeoxmYnVoL2uz2i6E3RLOwZgeqHG=s64",
      "userId": "12180749557274197061"
     },
     "user_tz": 420
    },
    "id": "U6A5ysNIqCaN"
   },
   "outputs": [],
   "source": [
    "# replacing zero values with the mean of the column\n",
    "data['BMI'] = data['BMI'].replace(0,data['BMI'].mean())#replacing 0 with mean of the bmi \n",
    "data['BloodPressure'] = data['BloodPressure'].replace(0,data['BloodPressure'].mean())#replacing 0 with mean of the Bloodpressure \n",
    "data['Glucose'] = data['Glucose'].replace(0,data['Glucose'].mean())##replacing 0 with mean of the Glucose\n",
    "data['Insulin'] = data['Insulin'].replace(0,data['Insulin'].mean())#replacing 0 with mean of the Insulin\n",
    "data['SkinThickness'] = data['SkinThickness'].replace(0,data['SkinThickness'].mean())#replacing 0 with mean of the Skinthickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZSY80M5qCaN",
    "outputId": "4b8735bc-b68f-4593-ae48-2e8b6bb96dde"
   },
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=9 :\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.histplot(data[column]) \n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlJ7qnbwqCaO"
   },
   "source": [
    "The data looks much better now than before. We will start our analysis with this data now as we don't want to lose important information.\n",
    "If our model doesn't work with accuracy, we will come back for more preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKXcRdUuqCaO"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to follow in feature selection\n",
    "1)Removing redundant columns-->one unique value columns,ids columns,serial no.\n",
    "2)Check for highly correlated features.If correlation between 2 numerical feature is more than 0.9,remove one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDzIsWc3qCaP",
    "outputId": "ea427c76-e3a0-4599-b14f-b4bc3d54b1f3"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data.drop('Outcome',axis=1).corr(),annot=True)# checking for correlation\n",
    "## NO correlated features are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23ULBkahqCaP",
    "outputId": "af292519-5509-43a2-c580-b0eea5b8a628"
   },
   "outputs": [],
   "source": [
    "## checking the duplicate rows\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRNKNqD5qCaP"
   },
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step:1 Define Dependant and indenpendant veriable\n",
    "X = data.iloc[:,:8]\n",
    "y = data.Outcome\n",
    "\n",
    "# Step:2 Create Taring and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "\n",
    "# Step:4 Model cration\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression() # Object Creation\n",
    "\n",
    "# Step:5 Fit the data\n",
    "LR.fit(X_train,y_train)\n",
    "\n",
    "# Step:6 Prdiction on test data\n",
    "y_predict = LR.predict(X_test)\n",
    "y_predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84 15]\n",
      " [19 36]]\n",
      "accuracy score 0.7792207792207793\n",
      "recall 0.6545454545454545\n",
      "precision 0.7058823529411765\n",
      "f1 score 0.679245283018868\n",
      "0.7515151515151515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83        99\n",
      "           1       0.71      0.65      0.68        55\n",
      "\n",
      "    accuracy                           0.78       154\n",
      "   macro avg       0.76      0.75      0.76       154\n",
      "weighted avg       0.78      0.78      0.78       154\n",
      "\n",
      "col_0     0   1\n",
      "Outcome        \n",
      "0        84  15\n",
      "1        19  36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,roc_auc_score,confusion_matrix,classification_report\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "\n",
    "# acaccuracy_score\n",
    "AC = accuracy_score(y_test,y_predict)\n",
    "print('accuracy score',AC)\n",
    "\n",
    "# recall\n",
    "recall = recall_score(y_test,y_predict)\n",
    "print('recall',recall)\n",
    "\n",
    "# precision\n",
    "precision = precision_score(y_test,y_predict)\n",
    "print(\"precision\",precision)\n",
    "\n",
    "# f1 score\n",
    "f1_score = f1_score(y_test,y_predict)\n",
    "print('f1 score',f1_score)\n",
    "\n",
    "# roc cure\n",
    "print(roc_auc_score(y_test,y_predict))\n",
    "\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test,y_predict))\n",
    "\n",
    "# cross tab\n",
    "print(pd.crosstab(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dazRdvSqCaU"
   },
   "source": [
    "## MULTICLASS CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wXukBYR4qCaV"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('iris.csv')# loading another dataset for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l9JJ9OwEqCaV",
    "outputId": "752b1a73-787f-406b-a20d-ca9f2c3290af",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()# first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-setosa        50\n",
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yLY2RWf6qCaV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=data.loc[:,['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]#independent variable \n",
    "y=data.Species#dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MqwX4ZSxqCaV"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)# training and testing  data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7GnB4xY0qCaV"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR=LogisticRegression(multi_class='ovr')# logistic regression for multiclass classification ovr-> one vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AslOA0eAqCaW",
    "outputId": "f5583636-4cdd-4393-cb98-2ae56200b220"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='ovr')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.fit(X_train,y_train)#  training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_U3vUpYYqCaW"
   },
   "outputs": [],
   "source": [
    "y_hat=LR.predict(X_test)# predicting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hZcsiZzcqCaW",
    "outputId": "51052739-e38a-4713-9570-ee2f44061d2a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Species</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iris-setosa</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iris-virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0            Iris-setosa  Iris-versicolor  Iris-virginica\n",
       "Species                                                      \n",
       "Iris-setosa               15                0               0\n",
       "Iris-versicolor            0               10               1\n",
       "Iris-virginica             0                0              12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test,y_hat) # confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "251ly1T_qCaW",
    "outputId": "87956c06-6df6-46e0-bad6-ae71dd2f3d4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall=recall_score(y_test,y_hat,average='weighted') #checking recall \n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JdyF50z8qCaW",
    "outputId": "20ebb7c3-8749-4dc6-b85f-92ffb758f985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9757085020242916"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision=precision_score(y_test,y_hat,average='weighted')# checking precision\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TPXiEmA-qCaW",
    "outputId": "a0686f30-b449-4903-b7c5-7e3454f5cc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.91      0.95        11\n",
      " Iris-virginica       0.92      1.00      0.96        12\n",
      "\n",
      "       accuracy                           0.97        38\n",
      "      macro avg       0.97      0.97      0.97        38\n",
      "   weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_hat))#recall,precision,f1 scores and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Logistic Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
